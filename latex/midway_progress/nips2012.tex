\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}

\usepackage{color}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{graphicx}

\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
% "define" Scala
\lstdefinelanguage{scala}{morekeywords={class,object,trait,extends,with,new,if,while,for,def,val,var,this},
otherkeywords={->,=>},
sensitive=true,
morecomment=[l]{//},
morecomment=[s]{/*}{*/},
morestring=[b]"}
% Default settings for code listings
\lstset{frame=tb,language=scala,aboveskip=3mm,belowskip=3mm,showstringspaces=false,columns=flexible,basicstyle={\small\ttfamily},
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/}}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09


\title{A Comparison of Parallel Algorithms:\\ {\Large Hogwild!, Shotgun and Distributed Averaging}}

\author{
David A. Leen \\
Department of Applied Mathematics\\
University of Washington \\
\texttt{dleen@uw.edu} \\
\And
Brian D. Walker \\
Department of Computer Science \& Engineering \\
University of Washington \\
\texttt{walker7734@gmail.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

\begin{abstract}
{\em Milestone progress:} We implemented the Hogwild! algorithm and demonstrated an approximately linear speedup over the
sequential stochastic gradient descent (SGD) algorithm.  We used the click prediction dataset and its sparse search token features to achieve a near optimal rate of convergence. We discuss the many issues arising with parallel algorithms and their implementations.
\end{abstract}

\section{Introduction}
We study three parallel algorithms: Hogwild! \cite{niu2011hogwild}, Shotgun \cite{bradley2011parallel} and distributed averaging \cite{zhang2012comunication}. Each approaches a similar problem in a distinct manner. Hogwild! and distributed averaging parallelize stochastic gradient descent (SGD) over samples, whereas Shotgun tackles the orthogonal problem of parallelizing $L_1$-regularized models over the features.

Hogwild! is a parallel version of SGD \cite{niu2011hogwild, zinkevich2010parallelized}. The algorithm is intended for problems on the order of several terabytes of data. It can be run quite effectively on inexpensive multi-core systems taking advantage of the low latency and high throughput of the data being stored in RAM or a RAID disk setup close to the processors. In these systems the bottlenecks arise from synchronizing and locking operations on the threads reading and writing shared memory. Regularization can also be applied in parallel settings \cite{langford2009slow, agarwal2011distributed}.

MapReduce in contrast deals with hundreds and thousands of terabytes of data but is not well suited for online, iterative algorithms like SGD. MapReduce suffers from low throughputs due to fault tolerance and redundancy. Distributed averaging is a simple version of a distributed optimization algorithm \cite{dekel2012optimal, agarwal2011distributed}. The method distributes the data samples evenly in group to some number of machines. Each machine performs a separate minimization of each group independently of one another. The estimates of the weights from each group are then averaged hence the name.

Shotgun provides a parallel solution to the high-dimensional problem with a large number of features using $L_1$-regularization \cite{ng2004feature}. In cases like this the dimensionality of the problem often dwarfs the number of samples. Once again the method involves taking a sequential algorithm, coordinate descent and simply parallelizes it, this time over features. The algorithm makes the coordinate updates in parallel to provide a speedup.

\section{Method}
The weight vector $w$ for logistic regression is updated according to the usual SGD equation:
\begin{equation}
w_i^{t+1} \leftarrow w_i^t + \eta \bigg[y^t - \frac{\exp{(w^t \cdot x^t)}}{1 + \exp{(w^t \cdot x^t)}} \bigg]x_i^t.
\end{equation}

 
\subsection{Proof of concept}
The minimal example demonstrating the Hogwild! concept in practice for a simple mapping is (in Scala for brevity):
\begin{lstlisting}
val m = collection.mutable.Map[Int, Double]() // create a mapping
for (i <- (0 until 10)) m.put(i, 0.0) // initialize key values to 0.0
for (i <- (0 until 20).par) m(i % 10) += 1.0 // update
\end{lstlisting}
The sequential version of this program gives a mapping \verb+0->2.0+, \verb+1->2.0+,\ldots. The parallel version (implemented from the \verb+.par+ method) produces a non-deterministic output with values either \verb+1.0+ or \verb+2.0+ i.e.: \verb+0->1.0+, \verb+1->2.0+, \verb+1->1.0+\ldots. Here we have experienced a race condition caused by threads overwriting one another. The Hogwild! algorithm relies on the sparsity of the data to ensure that updates do not operate simultaneously on the same keys. It relies on the same sparsity to parallelize the sequential algorithm.

\subsection{Algorithm}

\begin{algorithm}
  \caption{Hogwild! update step for a single processor}

  \begin{algorithmic}
  \Loop
  \State Randomly permute data
  \State Read current state of features $x^t$
  \For {i in $x^t$.keys}
  \State $w^{t+1}_i \leftarrow w_i^t - \eta x_i^t \nabla L(\mathbf{w}, \mathbf{x}, \mathbf{y})$
  \EndFor
  \EndLoop
  \end{algorithmic}
  \label{alg1}
\end{algorithm}

The component wise addition of elements from the weight vector is assumed to be atomic. In practice floats are not atomic and require extra care.

\section{Challenges and issues}
\label{gen_inst}
\subsection{Parallel and concurrent programming}
Parallel programming is hard. The Hogwild! solution is to ignore all the major issues associated with parallelism. 

The Hogwild! paper discusses a compromise between full synchronization and locking and no locking at all which they call AIG. AIG locks all the elements that are being updated in the for loop in algorithm \ref{alg1}. This is essentially what ConcurrentHashMap in Java does.

\subsection{Storing data in memory}
The Hogwild! algorithm relies on extremely fast throughput of data from memory or disk. Reading from disk is not fast enough to supply data as needed by a 4-6 core machine. This could be possible with a RAID setup but at the time this was not available.

The current solution is to load and parse all the data into RAM. 8-16GB of RAM is fairly standard these days, which is sufficient for the Click Prediction set, but it is not enough for terabyte data sets. We need to investigate how to break data sets into manageable chunks while still sampling uniformly as needed by the algorithms.

We suspect the Hogwild! paper uses sophisticated caching and data and memory management to process terabyte sized data on a single machine. 

\subsection{Non-sparse data}
A problem that we immediately noticed was whether Hogwild! can work on data that has both sparse and non-sparse components? The Click Prediction data has five dense values: age, gender, etc\ldots They are updated on every iteration. The tokens however are very sparse. Can we apply the Hogwild! algorithm in this case without ignoring the dense data?

\section{Results}
Initial results ranging from 1 to 4 CPU cores. During each run the same number of updates over the data occurs with the workload evenly distributed across each core. 

\includegraphics[scale=.3]{RMSE.jpg} \includegraphics[scale=.3]{running_time.jpg}

\section{Further work}
The next step in this project is to implement Shotgun for sparse logistic regression and distributed averaging for logistic regression. We will compare these algorithms amongst each other and to Hogwild! where appropriate. We will note where each algorithm performs best and where they fall short. We will have the baseline sequential predictions for each data set against which we can judge convergence and error rates.

We will then expand the algorithms from logistic regression to linear regression and support vector machines and potentially matrix completions. The data sets will be expanded from the Click Prediction set.

We will investigate the issue of non-sparse data contaminating the Hogwild! algorithm. We will investigate whether one can perform regularization during Hogwild!

\subsubsection*{References}

\begingroup
\renewcommand{\section}[2]{}%
%\renewcommand{\chapter}[2]{}% for other classes
\bibliographystyle{unsrt}
\bibliography{references}
\endgroup


\end{document}
