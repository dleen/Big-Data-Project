\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}


\usepackage{listings}
\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
% "define" Scala
\lstdefinelanguage{scala}{morekeywords={class,object,trait,extends,with,new,if,while,for,def,val,var,this},
otherkeywords={->,=>},
sensitive=true,
morecomment=[l]{//},
morecomment=[s]{/*}{*/},
morestring=[b]"}
% Default settings for code listings
\lstset{frame=tb,language=scala,aboveskip=3mm,belowskip=3mm,showstringspaces=false,columns=flexible,basicstyle={\small\ttfamily},
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/}}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09


\title{A Comparison of Parallel Algorithms:\\ {\Large Hogwild!, Shotgun and Distributed Averaging}}

\author{
David A. Leen \\
Department of Applied Mathematics\\
University of Washington \\
\texttt{dleen@uw.edu} \\
\And
Brian D. Walker \\
Computer Science \& Engineering \\
University of Washington \\
\texttt{walker7734@gmail.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

\begin{abstract}
{\em Milestone progress:} We implemented the Hogwild! algorithm and demonstrated an approximately linear speedup over the 
sequential stochastic gradient descent (SGD) algorithm.  We used the click prediction dataset and its sparse search token features to achieve a near optimal rate of convergence.
\end{abstract}

\section{Introduction}
Brief description of each algorithm. Context. Why we would use one over another? 

\section{Method}

\subsection{Proof of concept}
A fundamental example of the concept in Scala is:
\begin{lstlisting}
val m = collection.mutable.Map[Int, Double]() // create mapping
for (i <- (0 until 10)) m.put(i, 0.0) // initialize key values to 0.0
for (i <- (0 until 20).par) m(i % 10) += 1.0 // update 
\end{lstlisting}
For the non-parallel version of this program we expect a mapping  \verb+0->2.0+, \verb+1->2.0+,\ldots. This parallel version produces an output like \verb+0->1.0+, \verb+1->2.0+, \verb+1->1.0+\ldots. Where in some cases the threads have overwritten each other (hopefully not many in the full case).

This is the concept that we will use.

\subsection{Algorithm}
Some java code here demonstrating what we actually do.


\section{Challenges and issues}
\label{gen_inst}
\subsection{Parallel and concurrent programming}
Parallel programming is hard. The solution in this case is to ignore all the problems programming languages try to solve and just write. Atomic operations.

Atomic for int long, not for double in java?

Volatile.

Initialize the map.

Compromise by using array of doubles. Setting ahead of time.

AIG(?) compromise version is basically concurrenthashmap in java.



\subsection{Storing data in memory}
Load and parse all data into memory. Reading data bottleneck. Large amounts of memory available these days. 8GB standard.

Break data up into chunks. How to deal with this?

Caching?

Clever things.

\subsection{Non-sparse data}
Challenge: can this algorithm work on data that can be separated into sparse and non-sparse?

\section{Results}
\label{headings}
Graph of the speed up compared to sequential.

Graph of errors?

\subsection{Near linear speedup}
We accomplish a linear speedup over the sequential.

\section{Further work}
Implement Shotgun for logistic regression for sparse logistic regression. Then implement distributed averaging. {\bf Then} move onto new datasets.


\subsubsection*{References}
More papers?
\cite{niu2011hogwild}
\cite{bradley2011parallel}
\cite{zhang2012comunication}

\begingroup
\renewcommand{\section}[2]{}%
%\renewcommand{\chapter}[2]{}% for other classes
\bibliographystyle{unsrt}
\bibliography{references}	
\endgroup


\end{document}
