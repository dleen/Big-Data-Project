\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}

\usepackage{color}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listings}


\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
% "define" Scala
\lstdefinelanguage{scala}{morekeywords={class,object,trait,extends,with,new,if,while,for,def,val,var,this},
otherkeywords={->,=>},
sensitive=true,
morecomment=[l]{//},
morecomment=[s]{/*}{*/},
morestring=[b]"}
% Default settings for code listings
\lstset{frame=tb,language=scala,aboveskip=3mm,belowskip=3mm,showstringspaces=false,columns=flexible,basicstyle={\small\ttfamily},
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/}}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09


\title{A Comparison of Parallel Algorithms:\\ {\Large Hogwild!, Shotgun and Distributed Averaging}}

\author{
David A. Leen \\
Department of Applied Mathematics\\
University of Washington \\
\texttt{dleen@uw.edu} \\
\And
Brian D. Walker \\
Computer Science \& Engineering \\
University of Washington \\
\texttt{walker7734@gmail.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

\begin{abstract}
{\em Milestone progress:} We implemented the Hogwild! algorithm and demonstrated an approximately linear speedup over the
sequential stochastic gradient descent (SGD) algorithm.  We used the click prediction dataset and its sparse search token features to achieve a near optimal rate of convergence.
\end{abstract}

\section{Introduction}
We study three parallel algorithms: Hogwild! \cite{niu2011hogwild}, Shotgun \cite{bradley2011parallel} and distributed averaging \cite{zhang2012comunication}. Each approaches a similar problem in a distinct manner. Hogwild! and distributed averaging parallelize stochastic gradient descent (SGD) over samples, whereas Shotgun tackles the orthogonal problem of parallelizing $L_1$-regularized models over the features.



Brief description of each algorithm. Context. Why we would use one over another?

\section{Method}
The weight vector $w$ for logistic regression is updated according to the usual SGD equation:
\begin{equation}
w_i^{t+1} \leftarrow w_i^t + \eta \bigg[y^t - \frac{\exp{(w^t \cdot x^t)}}{1 + \exp{(w^t \cdot x^t)}} \bigg]x_i^t.
\end{equation}

 
\subsection{Proof of concept}
The minimal example demonstrating the Hogwild! concept in practice for a simple mapping is (in Scala for brevity):
\begin{lstlisting}
val m = collection.mutable.Map[Int, Double]() // create a mapping
for (i <- (0 until 10)) m.put(i, 0.0) // initialize key values to 0.0
for (i <- (0 until 20).par) m(i % 10) += 1.0 // update
\end{lstlisting}
The sequential version of this program gives a mapping \verb+0->2.0+, \verb+1->2.0+,\ldots. The parallel version (implemented from the \verb+.par+ method) produces a non-deterministic output with values either \verb+1.0+ or \verb+2.0+ i.e.: \verb+0->1.0+, \verb+1->2.0+, \verb+1->1.0+\ldots. Here we have experienced a race condition caused by threads overwriting one another. The Hogwild! algorithm relies on the sparsity of the data to ensure that updates do not operate simultaneously on the same keys. It relies on the same sparsity to parallelize the sequential algorithm.

\subsection{Algorithm}
Some java code here demonstrating what we actually do. \textcolor{red}{[BRIAN UPDATE THIS WITH SOME CODE OR SOMETHING THANKS !!:)]}.
\begin{algorithm}
  \caption{Hogwild! update step for a single processor}

  \begin{algorithmic}
  \Loop
  \State Randomly permute data
  \State Read current state of features $x^t$
  \For {i in $x^t$.keys}
  \State $w^{t+1}_i \leftarrow w_i^t - \eta x_i^t \nabla L(\mathbf{w}, \mathbf{x}, \mathbf{y})$
  \EndFor
  \EndLoop
  \end{algorithmic}
\end{algorithm}
The component wise addition of elements from the weight vector is assumed to be atomic. In practice floats are not atomic and require extra care.

\section{Challenges and issues}
\label{gen_inst}
\subsection{Parallel and concurrent programming}
Parallel programming is hard. The solution in this case is to ignore all the problems programming languages try to solve and just write. Atomic operations.

Atomic for int long, not for double in java?

Volatile.

Initialize the map.

Compromise by using array of doubles. Setting ahead of time.

AIG(?) compromise version is basically concurrenthashmap in java.



\subsection{Storing data in memory}
Load and parse all data into memory. Reading data bottleneck. Large amounts of memory available these days. 8GB standard.

Break data up into chunks. How to deal with this?

Caching?

Clever things.

\subsection{Non-sparse data}
Challenge: can this algorithm work on data that can be separated into sparse and non-sparse?

\section{Results}
\label{headings}
Graph of the speed up compared to sequential.

Graph of errors?

\subsection{Near linear speedup}
We accomplish a linear speedup over the sequential.

\section{Further work}
The next step in this project is to implement both Shotgun for sparse logistic regression and distributed averaging for logistic regression. We will compare these algorithms amongst each other and to Hogwild! where appropriate. We will note where each algorithm performs best and where they fall short. We will have the baseline sequential predictions for each data set against which we can judge convergence and error rates.

We will then expand the algorithms from logistic regression to linear regression and support vector machines and potentially matrix completions. The data sets will be expanded from the Click Prediction set to 


\subsubsection*{References}

\begingroup
\renewcommand{\section}[2]{}%
%\renewcommand{\chapter}[2]{}% for other classes
\bibliographystyle{unsrt}
\bibliography{references}
\endgroup


\end{document}
